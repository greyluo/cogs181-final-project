{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_forward_pass(model, input_shape=(1, 3, 64, 64), device='cuda'):\n",
    "    \"\"\"\n",
    "    Test a model with dummy data to ensure the forward pass works.\n",
    "    \n",
    "    Args:\n",
    "        model: The PyTorch model to test\n",
    "        input_shape: The shape of the input tensor (batch_size, channels, height, width)\n",
    "        device: The device to run the test on\n",
    "        \n",
    "    Returns:\n",
    "        output: The model output\n",
    "        output_shape: The shape of the output\n",
    "    \"\"\"\n",
    "    # Create a random tensor with the correct shape\n",
    "    x = torch.randn(input_shape).to(device)\n",
    "    \n",
    "    # Move model to the device\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Perform forward pass\n",
    "    with torch.no_grad():\n",
    "        try:\n",
    "            output = model(x)\n",
    "            print(f\"Forward pass successful!\")\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "            print(f\"Output shape: {output.shape}\")\n",
    "            return output, output.shape\n",
    "        except Exception as e:\n",
    "            print(f\"Forward pass failed with error: {e}\")\n",
    "            return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Count the number of trainable parameters in a model.\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        \n",
    "    Returns:\n",
    "        int: Number of trainable parameters\n",
    "    \"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Testing ResNet-18 variants:\n",
      "Basic ResNet-18:\n",
      "Input shape: torch.Size([1, 3, 32, 32]), Output shape: torch.Size([1, 10])\n",
      "\n",
      "SE-ResNet-18:\n",
      "Input shape: torch.Size([1, 3, 32, 32]), Output shape: torch.Size([1, 10])\n",
      "\n",
      "CBAM-ResNet-18:\n",
      "Input shape: torch.Size([1, 3, 32, 32]), Output shape: torch.Size([1, 10])\n",
      "\n",
      "Parameter counts:\n",
      "ResNet-18: 11,173,962 parameters\n",
      "SE-ResNet-18: 11,261,002 parameters\n",
      "CBAM-ResNet-18: 11,261,786 parameters\n",
      "SE-ResNet-18 has 0.78% more parameters than base\n",
      "CBAM-ResNet-18 has 0.79% more parameters than base\n"
     ]
    }
   ],
   "source": [
    "#=============================================\n",
    "# 1. Basic Residual Block\n",
    "#=============================================\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Basic residual block with optional attention mechanism\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, \n",
    "                 attention_type=None, reduction=16):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        \n",
    "        # Conv layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, \n",
    "                              stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, \n",
    "                              stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Downsample if needed (for dimension matching in skip connection)\n",
    "        self.downsample = downsample\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention_type = attention_type\n",
    "        if attention_type == 'se':\n",
    "            self.attention = SEBlock(out_channels, reduction)\n",
    "        elif attention_type == 'cbam':\n",
    "            self.attention = CBAMBlock(out_channels, reduction)\n",
    "        else:\n",
    "            self.attention = None\n",
    "            \n",
    "        # For saving attention maps\n",
    "        self.attention_map = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        # First conv block\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        # Second conv block\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # Apply attention if specified\n",
    "        if self.attention_type == 'se':\n",
    "            out = self.attention(out)\n",
    "        elif self.attention_type == 'cbam':\n",
    "            out, self.attention_map = self.attention(out)\n",
    "        \n",
    "        # Apply downsample if needed\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        # Add skip connection\n",
    "        out += identity\n",
    "        out = F.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "#=============================================\n",
    "# 2. Squeeze and Excitation Block\n",
    "#=============================================\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation Block\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channels // reduction, channels, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        # Squeeze operation\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        # Excitation operation\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        # Scale the input\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "#=============================================\n",
    "# 3. CBAM Components\n",
    "#=============================================\n",
    "class ChannelAttention(nn.Module):\n",
    "    \"\"\"Channel attention module for CBAM\"\"\"\n",
    "    def __init__(self, channels, reduction=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        \n",
    "        # Shared MLP\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels // reduction, kernel_size=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels // reduction, channels, kernel_size=1, bias=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = self.mlp(self.avg_pool(x))\n",
    "        max_out = self.mlp(self.max_pool(x))\n",
    "        out = torch.sigmoid(avg_out + max_out)\n",
    "        return out\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    \"\"\"Spatial attention module for CBAM\"\"\"\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), \"Kernel size must be 3 or 7\"\n",
    "        padding = 3 if kernel_size == 7 else 1\n",
    "        \n",
    "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        out = torch.cat([avg_out, max_out], dim=1)\n",
    "        out = torch.sigmoid(self.conv(out))\n",
    "        return out\n",
    "\n",
    "class CBAMBlock(nn.Module):\n",
    "    \"\"\"Convolutional Block Attention Module\"\"\"\n",
    "    def __init__(self, channels, reduction=16, kernel_size=7):\n",
    "        super(CBAMBlock, self).__init__()\n",
    "        self.channel_att = ChannelAttention(channels, reduction)\n",
    "        self.spatial_att = SpatialAttention(kernel_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Channel attention first\n",
    "        channel_att = self.channel_att(x)\n",
    "        x = x * channel_att\n",
    "        \n",
    "        # Then spatial attention\n",
    "        spatial_att = self.spatial_att(x)\n",
    "        x = x * spatial_att\n",
    "        \n",
    "        # Return result and spatial attention map for visualization\n",
    "        return x, spatial_att\n",
    "\n",
    "#=============================================\n",
    "# 4. Deep CNN Base Network\n",
    "#=============================================\n",
    "class DeepCNN(nn.Module):\n",
    "    \"\"\"Base class for deep CNN architectures\"\"\"\n",
    "    def __init__(self, block, num_blocks, attention_type=None, num_classes=10):\n",
    "        super(DeepCNN, self).__init__()\n",
    "        self.attention_type = attention_type\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial layers\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        # Classification head\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # For storing attention maps\n",
    "        self.attention_maps = []\n",
    "    \n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        # First block might need downsampling\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample, \n",
    "                           attention_type=self.attention_type))\n",
    "        \n",
    "        # Update input channels for next layers\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        # Add remaining blocks\n",
    "        for _ in range(1, num_blocks):\n",
    "            layers.append(block(self.in_channels, out_channels, 1, None, \n",
    "                               attention_type=self.attention_type))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Clear previous attention maps\n",
    "        self.attention_maps = []\n",
    "        \n",
    "        # Initial layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Residual layers\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        # Collect attention maps if using CBAM\n",
    "        if self.attention_type == 'cbam':\n",
    "            for name, module in self.named_modules():\n",
    "                if isinstance(module, ResidualBlock) and hasattr(module, 'attention_map') and module.attention_map is not None:\n",
    "                    self.attention_maps.append(module.attention_map)\n",
    "        \n",
    "        # Classification\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def get_attention_maps(self):\n",
    "        \"\"\"Method to access collected attention maps\"\"\"\n",
    "        return self.attention_maps\n",
    "\n",
    "#=============================================\n",
    "# 5. Model Instantiation Functions\n",
    "#=============================================\n",
    "def DeepResNet18(num_classes=10):\n",
    "    \"\"\"Deep ResNet-18 without attention\"\"\"\n",
    "    return DeepCNN(ResidualBlock, [2, 2, 2, 2], attention_type=None, num_classes=num_classes)\n",
    "\n",
    "def DeepSENet18(num_classes=10):\n",
    "    \"\"\"Deep ResNet-18 with SE blocks\"\"\"\n",
    "    return DeepCNN(ResidualBlock, [2, 2, 2, 2], attention_type='se', num_classes=num_classes)\n",
    "\n",
    "def DeepCBAMNet18(num_classes=10):\n",
    "    \"\"\"Deep ResNet-18 with CBAM blocks\"\"\"\n",
    "    return DeepCNN(ResidualBlock, [2, 2, 2, 2], attention_type='cbam', num_classes=num_classes)\n",
    "#=============================================\n",
    "# 6. Helper Functions\n",
    "#=============================================\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of trainable parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def test_model_architectures():\n",
    "    \"\"\"Test the model architectures and compare parameter counts\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Create ResNet-18 variants\n",
    "    base_model = DeepResNet18()\n",
    "    se_model = DeepSENet18()\n",
    "    cbam_model = DeepCBAMNet18()\n",
    "    \n",
    "    # Test forward pass\n",
    "    x = torch.randn(1, 3, 32, 32).to(device)\n",
    "    \n",
    "    # Move models to device\n",
    "    base_model = base_model.to(device)\n",
    "    se_model = se_model.to(device)\n",
    "    cbam_model = cbam_model.to(device)\n",
    "    \n",
    "    # Evaluation mode\n",
    "    base_model.eval()\n",
    "    se_model.eval()\n",
    "    cbam_model.eval()\n",
    "    \n",
    "    # Forward passes\n",
    "    with torch.no_grad():\n",
    "        # ResNet-18 variants\n",
    "        print(\"\\nTesting ResNet-18 variants:\")\n",
    "        print(\"Basic ResNet-18:\")\n",
    "        out_base = base_model(x)\n",
    "        print(f\"Input shape: {x.shape}, Output shape: {out_base.shape}\")\n",
    "        \n",
    "        print(\"\\nSE-ResNet-18:\")\n",
    "        out_se = se_model(x)\n",
    "        print(f\"Input shape: {x.shape}, Output shape: {out_se.shape}\")\n",
    "        \n",
    "        print(\"\\nCBAM-ResNet-18:\")\n",
    "        out_cbam = cbam_model(x)\n",
    "        print(f\"Input shape: {x.shape}, Output shape: {out_cbam.shape}\")\n",
    "        \n",
    "        # Parameter counts\n",
    "        print(\"\\nParameter counts:\")\n",
    "        base_params = count_parameters(base_model)\n",
    "        se_params = count_parameters(se_model)\n",
    "        cbam_params = count_parameters(cbam_model)\n",
    "        \n",
    "        print(f\"ResNet-18: {base_params:,} parameters\")\n",
    "        print(f\"SE-ResNet-18: {se_params:,} parameters\")\n",
    "        print(f\"CBAM-ResNet-18: {cbam_params:,} parameters\")\n",
    "        \n",
    "        print(f\"SE-ResNet-18 has {(se_params - base_params) / base_params * 100:.2f}% more parameters than base\")\n",
    "        print(f\"CBAM-ResNet-18 has {(cbam_params - base_params) / base_params * 100:.2f}% more parameters than base\")\n",
    "    \n",
    "test_model_architectures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cbam_model.png'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Create models\n",
    "base_model = DeepResNet18()\n",
    "se_model = DeepSENet18()\n",
    "cbam_model = DeepCBAMNet18()\n",
    "\n",
    "# Create sample input\n",
    "x = torch.randn(1, 3, 32, 32)\n",
    "\n",
    "# Visualize base model\n",
    "y_base = base_model(x)\n",
    "make_dot(y_base, params=dict(base_model.named_parameters())).render(\"base_model\", format=\"png\")\n",
    "\n",
    "# Visualize SE model\n",
    "y_se = se_model(x)\n",
    "make_dot(y_se, params=dict(se_model.named_parameters())).render(\"se_model\", format=\"png\")\n",
    "\n",
    "# Visualize CBAM model\n",
    "y_cbam = cbam_model(x)\n",
    "make_dot(y_cbam, params=dict(cbam_model.named_parameters())).render(\"cbam_model\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
