{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline CNN Training on Tiny ImageNet\n",
    "\n",
    "This notebook establishes a baseline CNN model for the Tiny ImageNet dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists.\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract_tiny_imagenet():\n",
    "    \"\"\"Download and extract the Tiny ImageNet dataset\"\"\"\n",
    "    os.makedirs('./data', exist_ok=True)\n",
    "    \n",
    "    if os.path.exists('./data/tiny-imagenet-200'):\n",
    "        print(\"Dataset already exists.\")\n",
    "        return\n",
    "    \n",
    "    url = 'http://pages.ucsd.edu/~ztu/courses/tiny-imagenet-200.zip'\n",
    "    print(\"Downloading Tiny ImageNet dataset...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    zip_path = './data/tiny-imagenet-200.zip'\n",
    "    with open(zip_path, 'wb') as f:\n",
    "        for chunk in tqdm(response.iter_content(chunk_size=1024*1024)):\n",
    "            if chunk:\n",
    "                f.write(chunk)\n",
    "    \n",
    "    print(\"Extracting Tiny ImageNet dataset...\")\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall('./data')\n",
    "    \n",
    "    os.remove(zip_path)\n",
    "    print(\"Download and extraction complete.\")\n",
    "\n",
    "# Download and extract dataset\n",
    "download_and_extract_tiny_imagenet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Dataset loaded with 100000 training images and 10000 validation images.\n",
      "Number of classes: 200\n"
     ]
    }
   ],
   "source": [
    "def prepare_tiny_imagenet():\n",
    "    \"\"\"Prepare the Tiny ImageNet dataset for PyTorch\"\"\"\n",
    "    # Define transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(64, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Paths to the data\n",
    "    data_dir = './data/tiny-imagenet-200'\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    \n",
    "    # Reorganize validation data if needed\n",
    "    val_img_dir = os.path.join(val_dir, 'images')\n",
    "    if os.path.exists(val_img_dir):\n",
    "        print(\"Reorganizing validation data...\")\n",
    "        val_dict = {}\n",
    "        with open(os.path.join(val_dir, 'val_annotations.txt'), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split('\\t')\n",
    "                val_dict[parts[0]] = parts[1]\n",
    "                \n",
    "        # Create class directories\n",
    "        for class_id in set(val_dict.values()):\n",
    "            os.makedirs(os.path.join(val_dir, class_id), exist_ok=True)\n",
    "            \n",
    "        # Move images to their respective class directories\n",
    "        for img, class_id in tqdm(val_dict.items(), desc=\"Organizing validation images\"):\n",
    "            if os.path.exists(os.path.join(val_img_dir, img)):\n",
    "                shutil.move(os.path.join(val_img_dir, img), \n",
    "                         os.path.join(val_dir, class_id, img))\n",
    "        \n",
    "        # Remove the images directory if it's empty\n",
    "        if os.path.exists(val_img_dir) and len(os.listdir(val_img_dir)) == 0:\n",
    "            os.rmdir(val_img_dir)\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    trainset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "    valset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "    \n",
    "    # Create data loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "    valloader = DataLoader(valset, batch_size=128, shuffle=False, num_workers=4)\n",
    "    \n",
    "    class_names = trainset.classes\n",
    "    \n",
    "    print(f\"Dataset loaded with {len(trainset)} training images and {len(valset)} validation images.\")\n",
    "    print(f\"Number of classes: {len(class_names)}\")\n",
    "    \n",
    "    return trainloader, valloader, class_names\n",
    "\n",
    "# Prepare data loaders\n",
    "trainloader, valloader, class_names = prepare_tiny_imagenet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Baseline CNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaselineCNN(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (10): ReLU(inplace=True)\n",
      "    (11): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (15): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=512, out_features=200, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class BaselineCNN(nn.Module):\n",
    "    \"\"\"Baseline CNN for Tiny ImageNet classification\"\"\"\n",
    "    def __init__(self, num_classes=200):\n",
    "        super(BaselineCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 64 -> 32\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 32 -> 16\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16 -> 8\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 8 -> 4\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Create model\n",
    "model = BaselineCNN(len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "# Summary of model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "        processed_size += batch_size\n",
    "    \n",
    "    epoch_loss = running_loss / processed_size\n",
    "    epoch_acc = running_corrects / processed_size\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            processed_size += batch_size\n",
    "    \n",
    "    epoch_loss = running_loss / processed_size\n",
    "    epoch_acc = running_corrects / processed_size\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc5a54eb4074f1595c0a3a3c35b92d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b6000f09ab4623bad647317d8c5949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.8457 Acc: 0.0337\n",
      "Val Loss: 4.3258 Acc: 0.0800\n",
      "New best model saved with accuracy: 0.0800\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccdef6a9532b428d9eea4f2528dab1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f14d04c7432c40fe8c6365e166f14f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 4.1772 Acc: 0.0973\n",
      "Val Loss: 4.1224 Acc: 0.0995\n",
      "New best model saved with accuracy: 0.0995\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0855163ac9c647bdaadc255407468cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m * \u001b[32m10\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Train and evaluate\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m train_loss, train_acc = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m val_loss, val_acc = evaluate(model, valloader, criterion)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Update learning rate\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Statistics\u001b[39;00m\n\u001b[32m     25\u001b[39m batch_size = inputs.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m running_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m * batch_size\n\u001b[32m     27\u001b[39m running_corrects += torch.sum(preds == labels.data).item()\n\u001b[32m     28\u001b[39m processed_size += batch_size\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# Initialize variables\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, valloader, criterion)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), 'model/baseline/best_model.pth')\n",
    "        print(f'New best model saved with accuracy: {best_acc:.4f}')\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Calculate training time\n",
    "time_elapsed = time.time() - since\n",
    "print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best val accuracy: {best_acc:.4f}')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), history['train_loss'], 'b-', label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), history['val_loss'], 'r-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), history['train_acc'], 'b-', label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-wise Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_accuracies(model, dataloader, class_names):\n",
    "    \"\"\"Calculate per-class accuracies\"\"\"\n",
    "    model.eval()\n",
    "    class_correct = [0] * len(class_names)\n",
    "    class_total = [0] * len(class_names)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Calculating class accuracies\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            correct = (preds == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    class_accuracies = {}\n",
    "    for i in range(len(class_names)):\n",
    "        if class_total[i] > 0:\n",
    "            class_accuracies[class_names[i]] = class_correct[i] / class_total[i]\n",
    "    \n",
    "    return class_accuracies\n",
    "\n",
    "# Get class accuracies\n",
    "class_accuracies = get_class_accuracies(model, valloader, class_names)\n",
    "\n",
    "# Print overall accuracy\n",
    "overall_acc = sum(class_accuracies.values()) / len(class_accuracies)\n",
    "print(f\"Overall accuracy: {overall_acc:.4f}\")\n",
    "\n",
    "# Find best and worst classes\n",
    "best_classes = sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "worst_classes = sorted(class_accuracies.items(), key=lambda x: x[1])[:5]\n",
    "\n",
    "print(\"\\nBest performing classes:\")\n",
    "for cls, acc in best_classes:\n",
    "    print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nWorst performing classes:\")\n",
    "for cls, acc in worst_classes:\n",
    "    print(f\"{cls}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'val_acc': best_acc,\n",
    "    'history': history\n",
    "}, 'model/baseline/final_model.pth')\n",
    "\n",
    "print(\"Model saved to 'final_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to load the model\n",
    "def load_model(model_path, model_class, num_classes):\n",
    "    # Initialize model\n",
    "    model = model_class(num_classes=num_classes)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "# Example usage (commented out to avoid execution)\n",
    "# loaded_model, checkpoint = load_model('final_model.pth', BaselineCNN, len(class_names))\n",
    "# print(f\"Loaded model with validation accuracy: {checkpoint['val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has established a baseline CNN model for the Tiny ImageNet dataset. You can use this as a foundation for more advanced architectures and experiments in your final project.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try different architectures (VGG, ResNet, etc.)\n",
    "2. Experiment with various hyperparameters (learning rate, batch size, etc.)\n",
    "3. Implement attention mechanisms\n",
    "4. Explore different optimization techniques\n",
    "5. Use more advanced regularization methods\n",
    "6. Test various data augmentation strategies\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
