{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Squeeze-and-Excitation Networks for Tiny ImageNet Classification\n",
    "\n",
    "This notebook tests a CNN model with CBAM (Convolutional Block Attention Module) on the Tiny ImageNet dataset, following the same structure as the baseline CNN implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import requests\n",
    "import zipfile\n",
    "import shutil\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating datasets...\n",
      "Dataset loaded with 100000 training images and 10000 validation images.\n",
      "Number of classes: 200\n"
     ]
    }
   ],
   "source": [
    "def prepare_tiny_imagenet():\n",
    "    \"\"\"Prepare the Tiny ImageNet dataset for PyTorch\"\"\"\n",
    "    # Define transforms\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomCrop(64, padding=4),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Paths to the data\n",
    "    data_dir = './data/tiny-imagenet-200'\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    val_dir = os.path.join(data_dir, 'val')\n",
    "    \n",
    "    # Reorganize validation data if needed\n",
    "    val_img_dir = os.path.join(val_dir, 'images')\n",
    "    if os.path.exists(val_img_dir):\n",
    "        print(\"Reorganizing validation data...\")\n",
    "        val_dict = {}\n",
    "        with open(os.path.join(val_dir, 'val_annotations.txt'), 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split('\\t')\n",
    "                val_dict[parts[0]] = parts[1]\n",
    "                \n",
    "        # Create class directories\n",
    "        for class_id in set(val_dict.values()):\n",
    "            os.makedirs(os.path.join(val_dir, class_id), exist_ok=True)\n",
    "            \n",
    "        # Move images to their respective class directories\n",
    "        for img, class_id in tqdm(val_dict.items(), desc=\"Organizing validation images\"):\n",
    "            if os.path.exists(os.path.join(val_img_dir, img)):\n",
    "                shutil.move(os.path.join(val_img_dir, img), \n",
    "                         os.path.join(val_dir, class_id, img))\n",
    "        \n",
    "        # Remove the images directory if it's empty\n",
    "        if os.path.exists(val_img_dir) and len(os.listdir(val_img_dir)) == 0:\n",
    "            os.rmdir(val_img_dir)\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    trainset = datasets.ImageFolder(train_dir, transform=transform_train)\n",
    "    valset = datasets.ImageFolder(val_dir, transform=transform_val)\n",
    "    \n",
    "    # Create data loaders\n",
    "    trainloader = DataLoader(trainset, batch_size=128, shuffle=True, num_workers=4)\n",
    "    valloader = DataLoader(valset, batch_size=128, shuffle=False, num_workers=4)\n",
    "    \n",
    "    class_names = trainset.classes\n",
    "    \n",
    "    print(f\"Dataset loaded with {len(trainset)} training images and {len(valset)} validation images.\")\n",
    "    print(f\"Number of classes: {len(class_names)}\")\n",
    "    \n",
    "    return trainloader, valloader, class_names\n",
    "\n",
    "# Prepare data loaders\n",
    "trainloader, valloader, class_names = prepare_tiny_imagenet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Baseline CNN Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENet(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): SELayer(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=4, bias=False)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=4, out_features=64, bias=False)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): ReLU(inplace=True)\n",
      "    (8): SELayer(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=8, bias=False)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=8, out_features=128, bias=False)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (14): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): SELayer(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=16, bias=False)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=16, out_features=256, bias=False)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (18): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (23): ReLU(inplace=True)\n",
      "    (24): SELayer(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=512, out_features=32, bias=False)\n",
      "        (1): ReLU(inplace=True)\n",
      "        (2): Linear(in_features=32, out_features=512, bias=False)\n",
      "        (3): Sigmoid()\n",
      "      )\n",
      "    )\n",
      "    (25): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "    (1): Flatten(start_dim=1, end_dim=-1)\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Dropout(p=0.5, inplace=False)\n",
      "    (5): Linear(in_features=512, out_features=200, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SELayer(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation attention module\"\"\"\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(channel // reduction, channel, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "class SENet(nn.Module):\n",
    "    \"\"\"CNN with Squeeze-and-Excitation attention for Tiny ImageNet classification\"\"\"\n",
    "    def __init__(self, num_classes=200):\n",
    "        super(SENet, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SELayer(64),  # Add SE layer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 64 -> 32\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SELayer(128),  # Add SE layer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 32 -> 16\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SELayer(256),  # Add SE layer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 16 -> 8\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            SELayer(512),  # Add SE layer\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # 8 -> 4\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:  # Check if bias exists before initializing\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Create model\n",
    "model = SENet(len(class_names))\n",
    "model = model.to(device)\n",
    "\n",
    "# Summary of model\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_se_attention(model, images, labels, class_names):\n",
    "    \"\"\"Visualize SE attention weights\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Get an SE layer from the model\n",
    "    se_layer = model.features[3]  # First SE layer\n",
    "    \n",
    "    # Storage for attention weights\n",
    "    attention_weights = []\n",
    "    \n",
    "    # Define a hook to capture attention weights\n",
    "    def hook_fn(module, input, output):\n",
    "        # SE layer returns x * y, where y contains the attention weights\n",
    "        # We want to capture the sigmoid output before multiplication\n",
    "        # This is the channel attention vector\n",
    "        b, c, _, _ = input[0].size()\n",
    "        attention = module.fc(module.avg_pool(input[0]).view(b, c))\n",
    "        attention_weights.append(attention.detach().cpu())\n",
    "    \n",
    "    # Register hook\n",
    "    hook = se_layer.fc.register_forward_hook(hook_fn)\n",
    "    \n",
    "    # Forward pass with selected images\n",
    "    num_images = min(4, images.size(0))\n",
    "    selected_images = images[:num_images].to(device)\n",
    "    selected_labels = labels[:num_images]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(selected_images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "    # Remove hook\n",
    "    hook.remove()\n",
    "    \n",
    "    # Get CPU predictions\n",
    "    preds = preds.cpu().numpy()\n",
    "    \n",
    "    # Visualize for each image\n",
    "    for i in range(num_images):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 2, 1)\n",
    "        img = images[i].cpu().numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = std * img + mean\n",
    "        img = np.clip(img, 0, 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Class: {class_names[labels[i]]}\\nPrediction: {class_names[preds[i]]}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Channel attention weights\n",
    "        plt.subplot(1, 2, 2)\n",
    "        weights = attention_weights[0][i].numpy()\n",
    "        plt.bar(range(len(weights)), weights)\n",
    "        plt.title('SE Channel Attention Weights')\n",
    "        plt.xlabel('Channel Index')\n",
    "        plt.ylabel('Weight Value')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'se_attention_viz_{i}.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    \n",
    "    for inputs, labels in tqdm(dataloader, desc=\"Training\"):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        batch_size = inputs.size(0)\n",
    "        running_loss += loss.item() * batch_size\n",
    "        running_corrects += torch.sum(preds == labels.data).item()\n",
    "        processed_size += batch_size\n",
    "    \n",
    "    epoch_loss = running_loss / processed_size\n",
    "    epoch_acc = running_corrects / processed_size\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            batch_size = inputs.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            running_corrects += torch.sum(preds == labels.data).item()\n",
    "            processed_size += batch_size\n",
    "    \n",
    "    epoch_loss = running_loss / processed_size\n",
    "    epoch_acc = running_corrects / processed_size\n",
    "    \n",
    "    return epoch_loss, epoch_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "----------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ebf0399a944caa923c4ad9e9c345a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/782 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4faef0bef136474b97058a78f0f992f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.0888 Acc: 0.0163\n",
      "Val Loss: 4.8472 Acc: 0.0287\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Parent directory model/cbam does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m     best_acc = val_acc\n\u001b[32m     46\u001b[39m     best_model_wts = copy.deepcopy(model.state_dict())\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodel/cbam/best_model.pth\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mNew best model saved with accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     50\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/torch/serialization.py:943\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    940\u001b[39m _check_save_filelike(f)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_writer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m    944\u001b[39m         _save(\n\u001b[32m    945\u001b[39m             obj,\n\u001b[32m    946\u001b[39m             opened_zipfile,\n\u001b[32m   (...)\u001b[39m\u001b[32m    949\u001b[39m             _disable_byteorder_record,\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/torch/serialization.py:810\u001b[39m, in \u001b[36m_open_zipfile_writer\u001b[39m\u001b[34m(name_or_buffer)\u001b[39m\n\u001b[32m    808\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    809\u001b[39m     container = _open_zipfile_writer_buffer\n\u001b[32m--> \u001b[39m\u001b[32m810\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.12.5/lib/python3.12/site-packages/torch/serialization.py:781\u001b[39m, in \u001b[36m_open_zipfile_writer_file.__init__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    777\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m    778\u001b[39m         torch._C.PyTorchFileWriter(\u001b[38;5;28mself\u001b[39m.file_stream, _compute_crc32)\n\u001b[32m    779\u001b[39m     )\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m781\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_compute_crc32\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Parent directory model/cbam does not exist."
     ]
    }
   ],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Number of epochs\n",
    "num_epochs = 30\n",
    "\n",
    "# Initialize variables\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "history = {\n",
    "    'train_loss': [], 'train_acc': [],\n",
    "    'val_loss': [], 'val_acc': []\n",
    "}\n",
    "\n",
    "# Training loop\n",
    "since = time.time()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}')\n",
    "    print('-' * 10)\n",
    "    \n",
    "    # Train and evaluate\n",
    "    train_loss, train_acc = train_one_epoch(model, trainloader, criterion, optimizer)\n",
    "    val_loss, val_acc = evaluate(model, valloader, criterion)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "    \n",
    "    # Print epoch results\n",
    "    print(f'Train Loss: {train_loss:.4f} Acc: {train_acc:.4f}')\n",
    "    print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "    \n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        torch.save(model.state_dict(), 'model/seblock/best_model.pth')\n",
    "        print(f'New best model saved with accuracy: {best_acc:.4f}')\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Calculate training time\n",
    "time_elapsed = time.time() - since\n",
    "print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best val accuracy: {best_acc:.4f}')\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, num_epochs+1), history['train_loss'], 'b-', label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), history['val_loss'], 'r-', label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, num_epochs+1), history['train_acc'], 'b-', label='Training Accuracy')\n",
    "plt.plot(range(1, num_epochs+1), history['val_acc'], 'r-', label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class-wise Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_accuracies(model, dataloader, class_names):\n",
    "    \"\"\"Calculate per-class accuracies\"\"\"\n",
    "    model.eval()\n",
    "    class_correct = [0] * len(class_names)\n",
    "    class_total = [0] * len(class_names)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Calculating class accuracies\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            correct = (preds == labels).squeeze()\n",
    "            for i in range(len(labels)):\n",
    "                label = labels[i].item()\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "    \n",
    "    # Calculate accuracies\n",
    "    class_accuracies = {}\n",
    "    for i in range(len(class_names)):\n",
    "        if class_total[i] > 0:\n",
    "            class_accuracies[class_names[i]] = class_correct[i] / class_total[i]\n",
    "    \n",
    "    return class_accuracies\n",
    "\n",
    "# Get class accuracies\n",
    "class_accuracies = get_class_accuracies(model, valloader, class_names)\n",
    "\n",
    "# Print overall accuracy\n",
    "overall_acc = sum(class_accuracies.values()) / len(class_accuracies)\n",
    "print(f\"Overall accuracy: {overall_acc:.4f}\")\n",
    "\n",
    "# Find best and worst classes\n",
    "best_classes = sorted(class_accuracies.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "worst_classes = sorted(class_accuracies.items(), key=lambda x: x[1])[:5]\n",
    "\n",
    "print(\"\\nBest performing classes:\")\n",
    "for cls, acc in best_classes:\n",
    "    print(f\"{cls}: {acc:.4f}\")\n",
    "\n",
    "print(\"\\nWorst performing classes:\")\n",
    "for cls, acc in worst_classes:\n",
    "    print(f\"{cls}: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Load Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'val_acc': best_acc,\n",
    "    'history': history\n",
    "}, 'model/seblock/final_model.pth')\n",
    "\n",
    "print(\"Model saved to 'final_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to load the model\n",
    "def load_model(model_path, model_class, num_classes):\n",
    "    # Initialize model\n",
    "    model = model_class(num_classes=num_classes)\n",
    "    \n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "# Example usage (commented out to avoid execution)\n",
    "# loaded_model, checkpoint = load_model('final_model.pth', BaselineCNN, len(class_names))\n",
    "# print(f\"Loaded model with validation accuracy: {checkpoint['val_acc']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has established a baseline CNN model for the Tiny ImageNet dataset. You can use this as a foundation for more advanced architectures and experiments in your final project.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try different architectures (VGG, ResNet, etc.)\n",
    "2. Experiment with various hyperparameters (learning rate, batch size, etc.)\n",
    "3. Implement attention mechanisms\n",
    "4. Explore different optimization techniques\n",
    "5. Use more advanced regularization methods\n",
    "6. Test various data augmentation strategies\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
